# References:
# (1) https://github.com/VincentStimper/normalizing-flows/blob/master/normflows/core.py

# May embed this part into the final model

import torch
import torch.nn as nn
import numpy as np
from pyro.distributions.torch_transform import TransformModule
from typing import List

from flowsum.nf.transforms import PyroSpline


class NormalizingFlowModel(nn.Module):
    """This class is equivalent to LegacyNormalizingFlowModel, except that it is built on top of the Pyro package,
    whereas the LegacyNormalizingFlowModel class is built on top of the functions I wrote by myself.
    """

    def __init__(self, transforms: List[TransformModule]) -> None:
        super().__init__()
        self.transforms = nn.ModuleList(
            transforms
        )  # we need to register all the parameters in the transforms

    def forward(self, mu, log_sigma):
        """Get a sample batch from a normalizing flow distribution transformed from N(mu, sigma).

        Bugs:
            (1) Bug 1 -- When running with PyroSpline and PyroSplingCoupling, the
                log_q values are positive. Whereas in Planar, Radial, and Sylvester,
                the values are negative.

                Solution: Positive is not an issue, since we are modeling the density, instead of probability.
                    Density doesn't have to be between 0 and 1.
                    - [TODO] Need to check the logabsdet calculation in the linear case of _monotonic_rational_spline().

            (2) Bug 2 -- log_prior.requires_grad = z.requires_grad = False, for
                all transformations.

                Solution: Use DiagGaussian to get the z_0 sample and use for loop to get the final z_K.
                    It seems that dist.sample() will get rid of gradient calculation.
                    - Checking the legacy version, I found that both z and log_prior there have requires_grad = True
                    - In order to backpropogate to mu and log_sigma, z should have graident.

            (3) Bug 3 -- After fixing Bug 2, when running with spline, transform.log_abs_det_jacobian(z, z_) gives
                (batch_size, latent_size). The logabsdet is generated by _monotonic_rational_spline(), which has the
                same shape as the inputs.

                Solution: If the transform is from PyroSpline or PyroSplineCoupling, then do a sum of the log_det.


        Args:
            mu: (batch_size, nf_latent_size)
            log_sigma: (batch_size, nf_latent_size)

        Returns:
          z: z_K
          log_q: log q_K(z_K)
          log_prior: log p(z_K|x)

        Notes:
            (1) The case where transforms = [] corresponds to the traditional variational inference where
                the latent distribution is characterized by diagonal Gaussian.
            (2) dist.TransformedDistribution is a child class of torch.distributions.transformed_distribution.TransformedDistribution,
                whose properties can be found at https://pytorch.org/docs/master/distributions.html#transformeddistribution.
            (3) dist.Normal is a child class of torch.distributions.distribution.Distribution, whose properties can be found at
                https://pytorch.org/docs/stable/distributions.html#distribution.
        """
        z, log_q = DiagGaussian(mu=mu, log_sigma=log_sigma)  # guarantee backpropogation

        for transform in self.transforms:
            z_ = transform(z)
            log_det = transform.log_abs_det_jacobian(z, z_)
            if isinstance(transform, PyroSpline):
                log_det = log_det.sum(-1)  # (batch_size, )
            assert log_det.shape == (mu.shape[0],)  # [TODO] remove after debugging
            log_q -= log_det  # (batch_size, )
            z = z_  # (batch_size, latent_size)

        log_prior = -0.5 * z.shape[-1] * np.log(2 * np.pi) - torch.sum(
            0.5 * torch.pow(z, 2), dim=-1
        )  # (batch_size, ), with prior being N(0, I)
        assert log_q.shape == log_prior.shape == (mu.shape[0],)

        return z, log_q, log_prior


class LegacyNormalizingFlowModel(nn.Module):
    """Legacy Normalizing Flow Model. Built on top of self-written normalizing flows."""

    def __init__(self, flows=None):
        """Constructor of normalizing flow model
        Args:
          prior: standard Gaussian
          flows: Flows to transform output of base encoder
          q0: Base Encoder
        """
        super().__init__()
        self.flows = nn.ModuleList(flows)

    def forward(self, mu, log_sigma):
        """Takes data batch, samples num_samples for each data point from base distribution
        Args:
            mu: (batch_size, nf_latent_size)
            log_sigma: (batch_size, nf_latent_size)

        Returns:
          z: z_K
          log_q: log q_K(z_K)
          log_prior: log p(z_K|x)

        Notes:
            The case where flows = [] corresponds to the traditional variational inference where
                the latent distribution is characterized by diagonal Gaussian.
        """
        z, log_q = DiagGaussian(mu=mu, log_sigma=log_sigma)
        for flow in self.flows:
            z, log_det = flow(z)
            log_q -= log_det  # z: (batch_size, latent_size), log_q: (batch_size, )
        log_prior = -0.5 * z.shape[-1] * np.log(2 * np.pi) - torch.sum(
            0.5 * torch.pow(z, 2), dim=-1
        )  # (batch_size, )
        return z, log_q, log_prior


def DiagGaussian(mu, log_sigma):
    assert mu.shape == log_sigma.shape  # (batch_size, nf_latent_size)
    eps = torch.randn(mu.shape, dtype=mu.dtype, device=mu.device)
    z = mu + torch.exp(log_sigma) * eps  # (batch_size, nf_latent_size)
    log_prob = -0.5 * mu.shape[-1] * np.log(2 * np.pi) - torch.sum(
        log_sigma + 0.5 * torch.pow(eps, 2), dim=-1
    )  # (batch_size, )
    return z, log_prob
